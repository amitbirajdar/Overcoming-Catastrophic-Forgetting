{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "IMM_Mean_L2.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "MYBlnSI0BJqu"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KSLIvmiq9hHS"
      },
      "source": [
        "'''Importing necessary libraries'''\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from numpy import random\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers, models\n",
        "from tensorflow.keras import utils as np_utils\n",
        "from tensorflow.python.framework.ops import disable_eager_execution\n",
        "\n",
        "from keras.datasets import mnist\n",
        "from keras.optimizers import SGD\n",
        "\n",
        "from sklearn.model_selection import train_test_split"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vGZVtOAUoEQ1"
      },
      "source": [
        "disable_eager_execution()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xOjv6Eli5f93"
      },
      "source": [
        "random.seed(7)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lnmblXIy9vib"
      },
      "source": [
        "'''loading MNIST dataset'''\n",
        "\n",
        "(x_trainA, y_trainA), (x_testA, y_testA) = mnist.load_data()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jse1kI9kpOXf"
      },
      "source": [
        "'''Concatenating into one single data for preprocessing'''\n",
        "\n",
        "X = np.concatenate((x_trainA,x_testA), axis=0)\n",
        "Y = np.concatenate((y_trainA,y_testA), axis=0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tJ0j7TMiqPBW"
      },
      "source": [
        "X.shape, Y.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jEuxeHm6v1P4"
      },
      "source": [
        "'''Normalizing the pixel values'''\n",
        "\n",
        "X = X / 255.0"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wFNsM2B-93ff"
      },
      "source": [
        "'''splitting MNIST into two partitions (0-4) and (5-9)'''\n",
        "\n",
        "'''Getting indexes '''\n",
        "idx0_4, idx5_9 = [], []\n",
        "\n",
        "for i in range(0,10):\n",
        "  if i < 5:\n",
        "    idx0_4.append(np.where(Y==i)) # filter mnist data for 0-4\n",
        "  else:\n",
        "    idx5_9.append(np.where(Y==i)) # filter mnist data for 5-9"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hB3JlIC3uKPb"
      },
      "source": [
        "'''Combining labels for the two partitions'''\n",
        "\n",
        "index0_4 = np.concatenate((idx0_4[0][0], idx0_4[1][0], idx0_4[2][0], idx0_4[3][0], idx0_4[4][0]), axis=0)\n",
        "index5_9 = np.concatenate((idx5_9[0][0], idx5_9[1][0], idx5_9[2][0], idx5_9[3][0], idx5_9[4][0]), axis=0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xixefae4y4ql"
      },
      "source": [
        "'''Getting labels'''\n",
        "\n",
        "y0_4 = Y[index0_4]\n",
        "y5_9 = Y[index5_9]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rt_Js5Qks_Eg"
      },
      "source": [
        "'''Getting corresponding images'''\n",
        "\n",
        "img0_4 = X[[index0_4]] # get mnist data for 0-4\n",
        "img5_9 = X[[index5_9]] # get mnist data for 5-9\n",
        "\n",
        "img0_4.shape, img5_9.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bEwF4mUmLwkr"
      },
      "source": [
        "'''Creating a MLP'''\n",
        "\n",
        "def create_model():\n",
        "  model = models.Sequential([\n",
        "      layers.Dense(800, activation='relu', input_dim=784),\n",
        "      layers.Dropout(0.5),\n",
        "      layers.Dense(800, activation='relu'),\n",
        "      layers.Dropout(0.5),\n",
        "      layers.Dense(800, activation='relu'),\n",
        "      layers.Dropout(0.5),\n",
        "      layers.Dense(5, activation='softmax'),\n",
        "      \n",
        "  ])\n",
        "  return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9oSBw92Jxcvi"
      },
      "source": [
        "'''Setting up optimizer'''\n",
        "\n",
        "from keras.optimizers import Adam\n",
        "opt = SGD(lr=0.05)\n",
        "batch_size = 64\n",
        "patience = 5\n",
        "clipgrad = 10000"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4t_T4ktQY5WB"
      },
      "source": [
        "'''Function for plotting training and validation accuracy'''\n",
        "\n",
        "def plot(history):\n",
        "  plt.plot(history.history['accuracy'])\n",
        "  plt.plot(history.history['val_accuracy'])\n",
        "  plt.legend(['acc', 'val_acc'])\n",
        "  plt.grid(True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OPFetTJ0Y_xA"
      },
      "source": [
        "'''Function to compile and fit model'''\n",
        "\n",
        "def compile_and_fit(model, xa, xb, validation_data, epochs, loss, filename):\n",
        "  model.compile(optimizer = opt, loss=loss, metrics=['accuracy'])\n",
        "  history = model.fit(xa, xb, epochs=epochs, validation_data=validation_data)\n",
        "  model.save_weights(f'/content/drive/MyDrive/552 PROJECT/IMM/{filename}.h5')\n",
        "  return model, history"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iI-vuHenlb8v"
      },
      "source": [
        "import numpy as np\n",
        "import keras.backend as K\n",
        "from keras.regularizers import Regularizer\n",
        "\n",
        "class L2reg(Regularizer):\n",
        "    def __init__(self, prior_weights, Lambda=0.1):\n",
        "        self.prior_weights = prior_weights\n",
        "        self.Lambda = Lambda\n",
        "\n",
        "    def __call__(self, x):\n",
        "        regularization = 0.\n",
        "        regularization = regularization + self.Lambda * K.sum(K.square(x - self.prior_weights))\n",
        "        return regularization\n",
        "\n",
        "    def get_config(self):\n",
        "        return {'Lambda': float(self.Lambda)}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hXMD3GcrvihV"
      },
      "source": [
        "# Task 1: Training on MNIST (0-4)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-9njd5HxH-nH"
      },
      "source": [
        "'''Setting up X and Y for the first task'''\n",
        "\n",
        "X0_4 = img0_4\n",
        "Y0_4 = y0_4"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_YgXjdWfv54R"
      },
      "source": [
        "'''Splitting the data into training and testing data'''\n",
        "\n",
        "x_train0_4, x_test0_4, y_train0_4, y_test0_4 = train_test_split(X0_4, Y0_4, test_size=0.2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mFCGMqPawY8v"
      },
      "source": [
        "x_train0_4.shape, y_train0_4.shape, x_test0_4.shape, y_test0_4.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tPRTGEvEtube"
      },
      "source": [
        "'''reshaping inputs to feed to dense layer of network'''\n",
        "\n",
        "x_train0_4 = x_train0_4.reshape((-1, 784))\n",
        "x_test0_4 = x_test0_4.reshape((-1, 784))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9drlTUFPDSSH"
      },
      "source": [
        "model0_4 = models.Sequential([\n",
        "    layers.Dense(800, input_dim = 784, activation='relu'),#, kernel_regularizer=L2reg(model0_4.weights[0]), bias_regularizer=L2reg(model0_4.weights[1])),\n",
        "    layers.Dropout(0.5),\n",
        "    layers.Dense(800, activation='relu'),#, kernel_regularizer = L2reg(model0_4.weights[2]), bias_regularizer = L2reg(model0_4.weights[3])),\n",
        "    layers.Dropout(0.5),\n",
        "    layers.Dense(800, activation='relu'),# kernel_regularizer = L2reg(model0_4.weights[4]), bias_regularizer = L2reg(model0_4.weights[5])),\n",
        "    layers.Dropout(0.5),\n",
        "    layers.Dense(5, activation='softmax'),# kernel_regularizer = L2reg(model0_4.weights[6]), bias_regularizer= L2reg(model0_4.weights[7]))\n",
        "])\n",
        "model0_4.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "jQ1_3J0fEY15"
      },
      "source": [
        "'''Compiling and training the model on task 1'''\n",
        "'''Save weights for initializing the next model'''\n",
        "\n",
        "model0_4.compile(loss='sparse_categorical_crossentropy', optimizer = opt, metrics=['accuracy'])\n",
        "\n",
        "history0_4 = model0_4.fit(x_train0_4, y_train0_4, epochs=50, validation_data = (x_test0_4, y_test0_4))\n",
        "\n",
        "model0_4.save('model0_4')\n",
        "model0_4.save_weights('/content/drive/MyDrive/552 PROJECT/IMM/task1mnist0_4.h5')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LFn5W_PKGKxY"
      },
      "source": [
        "plot(history0_4)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JyCZZy4Q3QNV"
      },
      "source": [
        "# Task 2: Training on MNIST (5-9)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zNKTTdxa3So-"
      },
      "source": [
        "'''Setting up X and Y for the second task'''\n",
        "\n",
        "X5_9 = img5_9\n",
        "Y5_9 = y5_9"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5iQkcrd0Rz88"
      },
      "source": [
        "'''Encoding labels 5-9 into 0-4 since the model's last dense (softmax) wont accept values starting from 5'''\n",
        "\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "encoder = LabelEncoder()\n",
        "y5_9 = encoder.fit_transform(Y5_9)\n",
        "y5_9"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yy26ZLgg4p14"
      },
      "source": [
        "Y5_9"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NLWwgfc-3b0r"
      },
      "source": [
        "'''Splitting the data into training and testing data'''\n",
        "\n",
        "x_train5_9, x_test5_9, y_train5_9, y_test5_9 = train_test_split(X5_9, y5_9, test_size=0.2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c7RqFm6d3bxi"
      },
      "source": [
        "x_train5_9.shape, y_train5_9.shape, x_test5_9.shape, y_test5_9.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8P9hI-Tr3bhF"
      },
      "source": [
        "'''reshaping inputs to feed to dense layer of network'''\n",
        "\n",
        "x_train5_9 = x_train5_9.reshape((-1, 784))\n",
        "x_test5_9 = x_test5_9.reshape((-1, 784))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1d2OIl0LfSpO"
      },
      "source": [
        "model5_9 = models.Sequential([\n",
        "    layers.Dense(800, input_dim = 784, activation='relu', kernel_regularizer=L2reg(model0_4.weights[0]), bias_regularizer=L2reg(model0_4.weights[1])),\n",
        "    layers.Dropout(0.5),\n",
        "    layers.Dense(800, activation='relu', kernel_regularizer = L2reg(model0_4.weights[2]), bias_regularizer = L2reg(model0_4.weights[3])),\n",
        "    layers.Dropout(0.5),\n",
        "    layers.Dense(800, activation='relu',kernel_regularizer = L2reg(model0_4.weights[4]), bias_regularizer = L2reg(model0_4.weights[5])),\n",
        "    layers.Dropout(0.5),\n",
        "    layers.Dense(5, activation='softmax',kernel_regularizer = L2reg(model0_4.weights[6]), bias_regularizer= L2reg(model0_4.weights[7]))\n",
        "])\n",
        "\n",
        "model5_9.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7L5YRGdS34Lj"
      },
      "source": [
        "'''compiling and training the model for task 2'''\n",
        "'''Load weights from the previous task so that same model is used'''\n",
        "'''Save weights for initializing the next model'''\n",
        "\n",
        "model5_9.compile(optimizer=opt, loss = 'sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "\n",
        "model5_9.load_weights('/content/drive/MyDrive/552 PROJECT/IMM/task1mnist0_4.h5')\n",
        "\n",
        "history5_9 = model5_9.fit(x_train5_9, y_train5_9, epochs=50, validation_data = (x_test5_9, y_test5_9))\n",
        "\n",
        "model5_9.save_weights('/content/drive/MyDrive/552 PROJECT/IMM/task2mnist5_9.h5')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PRPeF-ni34I5"
      },
      "source": [
        "'''Plotting the accuracy over training period'''\n",
        "\n",
        "plot(history5_9)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VAEgZ2kT33-z"
      },
      "source": [
        "'''checking predictions'''\n",
        "\n",
        "print(encoder.inverse_transform(model5_9.predict_classes(x_test5_9[0].reshape(-1,784))))\n",
        "plt.imshow(x_test5_9[0].reshape(28,28))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9yDn-1-ccr7j"
      },
      "source": [
        "# Task 3: training on shuffled (pixel shuffling) images (mnist 0-4)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jw3Qfz7YVgKD"
      },
      "source": [
        "'''Setting up X and Y for the third task'''\n",
        "\n",
        "pX0_4 = img0_4\n",
        "pY0_4 = y0_4"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DrvIYeSpe5tS"
      },
      "source": [
        "'''Shuffling the pixels'''\n",
        "\n",
        "i = np.arange(pX0_4.shape[1])\n",
        "np.random.shuffle(i)\n",
        "\n",
        "pX0_4 = pX0_4[:, i]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g1OWjmHqggko"
      },
      "source": [
        "'''visualizing the shuffled images'''\n",
        "\n",
        "plt.subplot(2,2,3)\n",
        "plt.imshow(img0_4[9883].reshape(28,28), cmap='gray')\n",
        "plt.xlabel(y0_4[9883])\n",
        "plt.xticks([])\n",
        "plt.yticks([])\n",
        "\n",
        "plt.subplot(2,2,4)\n",
        "plt.imshow(pX0_4[9883].reshape(28,28), cmap='gray')\n",
        "plt.xlabel(pY0_4[9883])\n",
        "plt.xticks([])\n",
        "plt.yticks([])\n",
        "\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GRPEwg-iWYyC"
      },
      "source": [
        "'''splitting data into training and testing data'''\n",
        "\n",
        "pX_train0_4, pX_test0_4, pY_train0_4, pY_test0_4 = train_test_split(pX0_4, pY0_4, test_size=0.2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1mzawtgBWzLq"
      },
      "source": [
        "'''reshaping inputs to feed to dense layer of network'''\n",
        "\n",
        "pX_train0_4 = pX_train0_4.reshape((-1, 784))\n",
        "pX_test0_4 = pX_test0_4.reshape((-1, 784))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7kkQttXPkm9l"
      },
      "source": [
        "'''creating a new model'''\n",
        "\n",
        "modelp0_4 = models.Sequential([\n",
        "    layers.Dense(800, input_dim = 784, activation='relu', kernel_regularizer=L2reg(model5_9.weights[0]), bias_regularizer=L2reg(model5_9.weights[1])),\n",
        "    layers.Dropout(0.5),\n",
        "    layers.Dense(800, activation='relu', kernel_regularizer = L2reg(model5_9.weights[2]), bias_regularizer = L2reg(model5_9.weights[3])),\n",
        "    layers.Dropout(0.5),\n",
        "    layers.Dense(800, activation='relu',kernel_regularizer = L2reg(model5_9.weights[4]), bias_regularizer = L2reg(model5_9.weights[5])),\n",
        "    layers.Dropout(0.5),\n",
        "    layers.Dense(5, activation='softmax',kernel_regularizer = L2reg(model5_9.weights[6]), bias_regularizer= L2reg(model5_9.weights[7]))\n",
        "])\n",
        "\n",
        "modelp0_4.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2mUpiNIwj_rQ"
      },
      "source": [
        "'''compiling and training the model for task 3'''\n",
        "'''Load weights from the previous task so that same model is used'''\n",
        "'''Save weights for initializing the next model'''\n",
        "\n",
        "modelp0_4.compile(loss='sparse_categorical_crossentropy', optimizer = opt, metrics=['accuracy'])\n",
        "\n",
        "modelp0_4.load_weights('/content/drive/MyDrive/552 PROJECT/IMM/task2mnist5_9.h5')\n",
        "\n",
        "historyp0_4 = modelp0_4.fit(pX_train0_4, pY_train0_4, epochs=50, validation_data = (pX_test0_4, pY_test0_4))\n",
        "\n",
        "modelp0_4.save_weights('/content/drive/MyDrive/552 PROJECT/IMM/task3pmnist0_4.h5')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0i0W5DajkyKt"
      },
      "source": [
        "'''Plotting the accuracy over training period'''\n",
        "\n",
        "plot(historyp0_4)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UblwRliKX5Gz"
      },
      "source": [
        "# Task 4: training on shuffled (pixel shuffling) images (mnist 5-9)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2T7sO1hSYCa7"
      },
      "source": [
        "'''Setting up X and Y for the second task'''\n",
        "\n",
        "pX5_9 = img5_9\n",
        "pY5_9 = y5_9"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cGVogkKhYCYE"
      },
      "source": [
        "'''Encoding labels 5-9 into 0-4 since the model's last dense (softmax) wont accept values starting from 5'''\n",
        "\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "encoder2 = LabelEncoder()\n",
        "pY5_9 = encoder2.fit_transform(pY5_9)\n",
        "pY5_9"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2FthSShQYCVk"
      },
      "source": [
        "'''Shuffling the pixels'''\n",
        "\n",
        "i = np.arange(pX5_9.shape[1])\n",
        "np.random.shuffle(i)\n",
        "\n",
        "pX5_9 = pX5_9[:, i]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "24ahl9-aYCS0"
      },
      "source": [
        "'''splitting data into training and testing data'''\n",
        "\n",
        "pX_train5_9, pX_test5_9, pY_train5_9, pY_test5_9 = train_test_split(pX5_9, pY5_9, test_size=0.2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2kdTk8drYCPT"
      },
      "source": [
        "'''reshaping inputs to feed to dense layer of network'''\n",
        "\n",
        "pX_train5_9 = pX_train5_9.reshape((-1, 784))\n",
        "pX_test5_9 = pX_test5_9.reshape((-1, 784))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m_nj7XHzYCNM"
      },
      "source": [
        "'''creating a new model'''\n",
        "\n",
        "modelp5_9 = models.Sequential([\n",
        "    layers.Dense(800, input_dim = 784, activation='relu', kernel_regularizer=L2reg(modelp0_4.weights[0]), bias_regularizer=L2reg(modelp0_4.weights[1])),\n",
        "    layers.Dropout(0.5),\n",
        "    layers.Dense(800, activation='relu', kernel_regularizer = L2reg(modelp0_4.weights[2]), bias_regularizer = L2reg(modelp0_4.weights[3])),\n",
        "    layers.Dropout(0.5),\n",
        "    layers.Dense(800, activation='relu',kernel_regularizer = L2reg(modelp0_4.weights[4]), bias_regularizer = L2reg(modelp0_4.weights[5])),\n",
        "    layers.Dropout(0.5),\n",
        "    layers.Dense(5, activation='softmax',kernel_regularizer = L2reg(modelp0_4.weights[6]), bias_regularizer= L2reg(modelp0_4.weights[7]))\n",
        "])\n",
        "\n",
        "modelp5_9.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jJPO6tWvYCKh"
      },
      "source": [
        "'''compiling and training the model for task 3'''\n",
        "'''Load weights from the previous task so that same model is used'''\n",
        "'''Save weights for initializing the next model'''\n",
        "\n",
        "\n",
        "modelp5_9.compile(loss='sparse_categorical_crossentropy', optimizer = opt, metrics=['accuracy'])\n",
        "\n",
        "modelp5_9.load_weights('/content/drive/MyDrive/552 PROJECT/IMM/task3pmnist0_4.h5')\n",
        "\n",
        "historyp5_9 = modelp5_9.fit(pX_train5_9, pY_train5_9, epochs=50, validation_data = (pX_test5_9, pY_test5_9))\n",
        "\n",
        "modelp5_9.save_weights('/content/drive/MyDrive/552 PROJECT/IMM/task4pmnist5_9.h5')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u8WkTpFSYCIh"
      },
      "source": [
        "'''Plotting the accuracy over training period'''\n",
        "\n",
        "plot(historyp5_9)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R5-NOWVbZ0dD"
      },
      "source": [
        "# Checking accuracy change on Task A with IMM_Mean"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Evi5Sb_DmjIN"
      },
      "source": [
        "'''creating dictionary to save model weights and bias by layer'''\n",
        "\n",
        "def create_parameter_dict(layers, params):  \n",
        "  model_dict = {1:[], 2:[], 3:[], 4:[]}\n",
        "  p = 0\n",
        "  for layer in range(1, layers+1): \n",
        "    model_dict[layer].append(params[p])\n",
        "    model_dict[layer].append(params[p+1])\n",
        "    p = p + 2\n",
        "  return model_dict"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4HIqXSI5nEu3"
      },
      "source": [
        "'''creating array of weights and bias for all models (weights format)'''\n",
        "\n",
        "def create_new_model(layers, models):\n",
        "  \n",
        "  no_of_models = len(models)\n",
        "  new_model = {}\n",
        "\n",
        "\n",
        "  # creating new model parameter dictionary\n",
        "  for layer in range(1, layers+1):\n",
        "    new_model[layer] = []\n",
        "  \n",
        "\n",
        "  for layer in range(1, layers+1): # loop layer by layer\n",
        "    weights = 0\n",
        "    for n in range(no_of_models): # summation of same layer for all models\n",
        "      weights = weights + models[n][layer][0]\n",
        "    \n",
        "    weights = weights / no_of_models\n",
        "    new_model[layer].append(weights)\n",
        "    \n",
        "\n",
        "  for layer in range(1, layers+1): # loop layer by layer\n",
        "    bias = 0\n",
        "    for n in range(no_of_models): # summation of same layer for all models\n",
        "      bias = bias + models[n][layer][1]    \n",
        "    \n",
        "    bias = bias / no_of_models\n",
        "    new_model[layer].append(bias)\n",
        "  \n",
        "  # returning new model weights\n",
        "  return new_model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NV6JY_8uZ48N"
      },
      "source": [
        "'''Creating model_weight dictionaries'''\n",
        "\n",
        "nlayers = 4\n",
        "\n",
        "params0_4 = model0_4.get_weights()\n",
        "model0_4_dict = create_parameter_dict(nlayers, params0_4)\n",
        "\n",
        "params5_9 = model5_9.get_weights()\n",
        "model5_9_dict = create_parameter_dict(nlayers, params5_9)\n",
        "\n",
        "paramsP0_4 = modelp0_4.get_weights()\n",
        "modelP0_4_dict = create_parameter_dict(nlayers, paramsP0_4)\n",
        "\n",
        "paramsP5_9 = modelp0_4.get_weights()\n",
        "modelP5_9_dict = create_parameter_dict(nlayers, paramsP5_9)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HdbvB8xenEsL"
      },
      "source": [
        "'''creating arrays of weights and biases'''\n",
        "\n",
        "after2tasks_model_dict = create_new_model(nlayers, [model0_4_dict, model5_9_dict])\n",
        "after2tasks_model = []\n",
        "for i in after2tasks_model_dict.keys():\n",
        "  after2tasks_model.append(after2tasks_model_dict[i][0])\n",
        "  after2tasks_model.append(after2tasks_model_dict[i][1])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y7KxrHJxnEo6"
      },
      "source": [
        "'''creating arrays of weights and biases'''\n",
        "\n",
        "after3tasks_model_dict = create_new_model(nlayers, [model0_4_dict, model5_9_dict, modelP0_4_dict])\n",
        "after3tasks_model = []\n",
        "for i in after3tasks_model_dict.keys():\n",
        "  after3tasks_model.append(after3tasks_model_dict[i][0])\n",
        "  after3tasks_model.append(after3tasks_model_dict[i][1])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vQwAV8OanEm1"
      },
      "source": [
        "'''creating arrays of weights and biases'''\n",
        "\n",
        "after4tasks_model_dict = create_new_model(nlayers, [model0_4_dict, model5_9_dict, modelP0_4_dict, modelP5_9_dict])\n",
        "after4tasks_model = []\n",
        "for i in after4tasks_model_dict.keys():\n",
        "  after4tasks_model.append(after4tasks_model_dict[i][0])\n",
        "  after4tasks_model.append(after4tasks_model_dict[i][1])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sTIecCnMpSbg"
      },
      "source": [
        "'''creating 3 new models for evaluating performance on Task A after training on:\n",
        "(i) Task B\n",
        "(ii) Task B and Task C\n",
        "(iii) Task B, Task C and Task D'''\n",
        "\n",
        "imm2 = create_model()\n",
        "imm3 = create_model()\n",
        "imm4 = create_model()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_nQbUANlzNlr"
      },
      "source": [
        "'''Initializing weights and biases for newly created models'''\n",
        "\n",
        "def set_model_weights(new_model, weights):\n",
        "  lc = 0\n",
        "  for i in range(0, 8, 2):\n",
        "    layer = []\n",
        "    layer.append(weights[i])\n",
        "    layer.append(weights[i+1])\n",
        "    new_model.layers[lc].set_weights(layer)\n",
        "    lc = lc + 2\n",
        "  return new_model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AkarqMNyzrvL"
      },
      "source": [
        "'''Initialize models with appropriate weights and biases'''\n",
        "\n",
        "imm2 = set_model_weights(imm2, after2tasks_model)\n",
        "imm3 = set_model_weights(imm3, after3tasks_model)\n",
        "imm4 = set_model_weights(imm4, after4tasks_model)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2U7gxNTmv_CL"
      },
      "source": [
        "'''evaluating accuracy after training on task b'''\n",
        "\n",
        "imm2.compile(loss='sparse_categorical_crossentropy', optimizer = opt, metrics=['accuracy'])\n",
        "imm2_accuracy = round(imm2.evaluate(x_test0_4, y_test0_4)[1]*100, 2)\n",
        "imm2_accuracy"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8ucvOyreyY6Q"
      },
      "source": [
        "'''evaluating accuracy after training on task b and task c'''\n",
        "\n",
        "imm3.compile(loss='sparse_categorical_crossentropy', optimizer = opt, metrics=['accuracy'])\n",
        "imm3_accuracy = round(imm3.evaluate(x_test0_4, y_test0_4)[1]*100, 2)\n",
        "imm3_accuracy"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5mG-g03zyrav"
      },
      "source": [
        "'''evaluating accuracy after training on task b, task c and task d'''\n",
        "\n",
        "imm4.compile(loss='sparse_categorical_crossentropy', optimizer = opt, metrics=['accuracy'])\n",
        "imm4_accuracy = round(imm4.evaluate(x_test0_4, y_test0_4)[1]*100, 2)\n",
        "imm4_accuracy"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H2nK-xlM1ue0"
      },
      "source": [
        "original_acc = round(model0_4.evaluate(x_test0_4, y_test0_4)[1]*100, 2)\n",
        "original_acc"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y1196Lwv18yU"
      },
      "source": [
        "accuracies = [original_acc, imm2_accuracy, imm3_accuracy, imm4_accuracy]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8vsbJ6itZ4tj"
      },
      "source": [
        "fig, ax = plt.subplots()\n",
        "\n",
        "fig.set_size_inches(7, 5, forward=True)\n",
        "\n",
        "x = ['Original Accuracy', 'After Task 2', 'After Task 3', 'After Task 4']\n",
        "y = accuracies\n",
        "\n",
        "# Save the chart so we can loop through the bars below.\n",
        "bars = ax.bar(\n",
        "    x = x,\n",
        "    height=y,\n",
        "    tick_label=x\n",
        ")\n",
        "\n",
        "# Axis formatting.\n",
        "ax.spines['top'].set_visible(True)\n",
        "ax.spines['right'].set_visible(True)\n",
        "ax.spines['left'].set_visible(True)\n",
        "ax.spines['bottom'].set_color('#DDDDDE')\n",
        "ax.tick_params(bottom=False, left=True)\n",
        "ax.set_axisbelow(True)\n",
        "ax.yaxis.grid(True, color='#EEEEEE')\n",
        "ax.xaxis.grid(True)\n",
        "\n",
        "# Add text annotations to the top of the bars.\n",
        "bar_color = bars[0].get_facecolor()\n",
        "for bar in bars:\n",
        "  ax.text(\n",
        "      bar.get_x() + bar.get_width() / 2,\n",
        "      bar.get_height() + 0.3,\n",
        "      round(bar.get_height(), 1),\n",
        "      horizontalalignment='center',\n",
        "      color='black'\n",
        "      #weight='bold'\n",
        "  )\n",
        "\n",
        "# Add labels and a title. Note the use of `labelpad` and `pad` to add some\n",
        "# extra space between the text and the tick labels.\n",
        "#ax.set_xlabel('Different methods', labelpad=15, color='#333333')\n",
        "ax.set_ylabel('Accuracy', labelpad=15, color='#333333')\n",
        "ax.set_title('Demonstration of Catastrophic Forgetting', pad=15, color='#333333',\n",
        "             weight='bold')\n",
        "\n",
        "fig.tight_layout()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rRsR8LDRbskH"
      },
      "source": [
        "\n",
        "plt.title('Accuracy of the model on task A')\n",
        "plt.plot([1,2,3,4], accuracies, color='blue', marker='o', linestyle='dashed', linewidth=2, markersize=12)\n",
        "plt.xticks([1,2,3,4])\n",
        "plt.xlabel('Tasks')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.grid(True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yaA21ubb2Do0"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}